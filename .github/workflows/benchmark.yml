name: 🚀 Performance Benchmarks

# Comprehensive benchmark tracking with performance regression detection
# Runs AFTER CI completes successfully to avoid duplication
# Features:
# - Detailed benchmark results with fancy GitHub UI output
# - Performance regression detection with alerts
# - Historical comparison and trend analysis
# - Multi-platform benchmark execution
# - Automatic baseline updates on main branch

on:
  push:
    branches: [main] # Only run comprehensive benchmarks on main branch
  pull_request:
    branches: [main, develop] # Run on PRs to main/develop for regression detection
  workflow_run:
    workflows: ["CI"] # Run after CI completes successfully to avoid duplication
    types:
      - completed
    branches: [main, develop, feature/*] # Run on feature branches only after CI passes
  workflow_dispatch: # Allow manual trigger for ad-hoc performance testing
    inputs:
      baseline_ref:
        description: "Git reference to use as baseline (default: main)"
        required: false
        default: "main"
        type: string

env:
  GO_VERSION: "1.24" # Keep consistent with ci.yml
  CACHE_VERSION: "v1" # Increment this to invalidate all caches

permissions:
  contents: read
  pull-requests: write # Required for PR comments
  actions: read
  checks: write

jobs:
  # Check if we should run benchmarks
  should-run:
    name: 🔍 Check if benchmarks should run
    runs-on: ubuntu-latest
    outputs:
      should-run: ${{ steps.check.outputs.should-run }}
    steps:
      - name: Check conditions
        id: check
        run: |
          echo "Event: ${{ github.event_name }}"
          echo "Ref: ${{ github.ref }}"

          # Always run on workflow_dispatch
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "Manual trigger - running benchmarks"
            echo "should-run=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Always run on direct push to main
          if [ "${{ github.event_name }}" = "push" ] && [ "${{ github.ref }}" = "refs/heads/main" ]; then
            echo "Push to main - running benchmarks"
            echo "should-run=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Always run on pull requests
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "Pull request - running benchmarks"
            echo "should-run=true" >> $GITHUB_OUTPUT
            exit 0
          fi

          # For workflow_run, only run if CI was successful
          if [ "${{ github.event_name }}" = "workflow_run" ]; then
            if [ "${{ github.event.workflow_run.conclusion }}" = "success" ]; then
              echo "CI workflow successful - running benchmarks"
              echo "should-run=true" >> $GITHUB_OUTPUT
            else
              echo "CI workflow failed/cancelled - skipping benchmarks"
              echo "should-run=false" >> $GITHUB_OUTPUT
            fi
            exit 0
          fi

          # Default to not running
          echo "Condition not met - skipping benchmarks"
          echo "should-run=false" >> $GITHUB_OUTPUT

  benchmark:
    name: 🏁 Performance Benchmarks
    runs-on: ubuntu-latest
    needs: should-run
    if: needs.should-run.outputs.should-run == 'true'
    strategy:
      matrix:
        # Test on different architectures for comprehensive performance analysis
        include:
          - os: ubuntu-latest
            goarch: amd64
            label: "Linux AMD64"
    steps:
      - name: 📥 Checkout current code
        uses: actions/checkout@v4
        with:
          # For workflow_run events, checkout the head SHA of the triggering workflow
          ref: ${{ github.event_name == 'workflow_run' && github.event.workflow_run.head_sha || github.sha }}
          fetch-depth: 0 # Fetch full history for baseline comparison

      - name: 🔧 Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: ${{ env.GO_VERSION }}

      - name: 💾 Cache Go modules and build cache
        uses: actions/cache@v4
        with:
          path: |
            ~/go/pkg/mod
            ~/.cache/go-build
          key: ${{ env.CACHE_VERSION }}-${{ runner.os }}-go-bench-${{ matrix.goarch }}-${{ hashFiles('**/go.sum') }}
          restore-keys: |
            ${{ env.CACHE_VERSION }}-${{ runner.os }}-go-bench-${{ matrix.goarch }}-
            ${{ env.CACHE_VERSION }}-${{ runner.os }}-go-bench-
            ${{ env.CACHE_VERSION }}-${{ runner.os }}-go-

      - name: 📦 Download dependencies
        run: go mod download

      - name: 🔧 Install benchmark tools
        run: |
          echo "Installing benchmark analysis tools..."
          go install golang.org/x/perf/cmd/benchstat@latest
          go install golang.org/x/perf/cmd/benchfilter@latest

          echo "✅ Installed benchmark tools:"
          echo "  - benchstat: Statistical analysis and A/B comparisons"
          echo "  - benchfilter: Benchmark result filtering"

      - name: 🏃 Run current benchmarks
        env:
          GOARCH: ${{ matrix.goarch }}
        run: |
          echo "🚀 Running benchmarks for ${{ matrix.label }}..."

          # Create results directory
          mkdir -p benchmark-results

          # Run benchmarks with extended output
          echo "Running comprehensive benchmark suite..."
          go test -bench=. -benchmem -benchtime=5s -count=3 ./... | tee "benchmark-results/current-${{ matrix.goarch }}.txt"

          # Also run with CPU profiling for detailed analysis
          echo "Running benchmarks with CPU profiling..."
          go test -bench=. -benchmem -cpuprofile="benchmark-results/cpu-${{ matrix.goarch }}.prof" ./... > /dev/null 2>&1 || true

          # Run memory profiling
          echo "Running benchmarks with memory profiling..."
          go test -bench=. -benchmem -memprofile="benchmark-results/mem-${{ matrix.goarch }}.prof" ./... > /dev/null 2>&1 || true

          # Generate benchmark statistics
          echo "📊 Current benchmark summary for ${{ matrix.label }}:"
          echo "================================================================"
          cat "benchmark-results/current-${{ matrix.goarch }}.txt" | grep "^Benchmark" | head -10
          echo "================================================================"

      - name: 📥 Get baseline benchmarks
        id: baseline
        run: |
          BASELINE_REF="${{ github.event.inputs.baseline_ref || 'main' }}"

          if [ "${{ github.event_name }}" = "pull_request" ]; then
            BASELINE_REF="${{ github.base_ref }}"
          fi

          echo "baseline_ref=$BASELINE_REF" >> $GITHUB_OUTPUT
          echo "🔍 Getting baseline benchmarks..."

          # Current branch/commit info
          current_branch=$(git branch --show-current)
          current_commit=$(git rev-parse HEAD)

          echo "Current branch: $current_branch"
          echo "Current commit: $current_commit"
          echo "Target baseline: $BASELINE_REF"

          # Strategy 1: Try to use existing baseline file from repository
          if [ -f ".github/benchmark-baselines/baseline-${{ matrix.goarch }}.txt" ] && [ -s ".github/benchmark-baselines/baseline-${{ matrix.goarch }}.txt" ]; then
            echo "✅ Found existing baseline file in repository"
            cp ".github/benchmark-baselines/baseline-${{ matrix.goarch }}.txt" "benchmark-results/baseline-${{ matrix.goarch }}.txt"
            echo "has_baseline=true" >> $GITHUB_OUTPUT
            echo "baseline_source=repository" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Strategy 2: If we're on the baseline branch already, skip baseline comparison
          if [ "$current_branch" = "$BASELINE_REF" ] || [ "${{ github.ref }}" = "refs/heads/$BASELINE_REF" ]; then
            echo "📝 Already on target baseline branch ($BASELINE_REF), skipping baseline comparison"
            echo "This run will establish new baseline after completion"
            touch "benchmark-results/baseline-${{ matrix.goarch }}.txt"
            echo "has_baseline=false" >> $GITHUB_OUTPUT
            echo "baseline_source=none" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Strategy 3: Try to checkout the baseline branch/commit
          echo "🔄 Attempting to checkout baseline: $BASELINE_REF"

          # Try different checkout strategies
          checkout_success=false

          # Try 1: Direct checkout
          if git checkout "$BASELINE_REF" 2>/dev/null; then
            echo "✅ Successfully checked out $BASELINE_REF"
            checkout_success=true
          # Try 2: Checkout with origin prefix  
          elif git checkout "origin/$BASELINE_REF" 2>/dev/null; then
            echo "✅ Successfully checked out origin/$BASELINE_REF"
            checkout_success=true
          # Try 3: Fetch and checkout
          elif git fetch origin "$BASELINE_REF:$BASELINE_REF" 2>/dev/null && git checkout "$BASELINE_REF" 2>/dev/null; then
            echo "✅ Successfully fetched and checked out $BASELINE_REF"
            checkout_success=true
          fi

          if [ "$checkout_success" = true ]; then
            echo "🏃 Running baseline benchmarks on $BASELINE_REF..."
            go test -bench=. -benchmem -benchtime=5s -count=3 ./... | tee "benchmark-results/baseline-${{ matrix.goarch }}.txt" || true
            
            # Return to current commit
            echo "🔄 Returning to original commit: $current_commit"
            git checkout "$current_commit"
            
            echo "has_baseline=true" >> $GITHUB_OUTPUT
            echo "baseline_source=branch" >> $GITHUB_OUTPUT
          else
            echo "⚠️ Could not checkout $BASELINE_REF using any strategy"
            echo "Proceeding without baseline comparison"
            touch "benchmark-results/baseline-${{ matrix.goarch }}.txt"
            echo "has_baseline=false" >> $GITHUB_OUTPUT
            echo "baseline_source=none" >> $GITHUB_OUTPUT
          fi

      - name: 📊 Generate performance analysis
        id: analysis
        if: steps.baseline.outputs.has_baseline == 'true'
        env:
          GOARCH: ${{ matrix.goarch }}
        run: |
          echo "🔬 Analyzing performance changes for ${{ matrix.label }}..."

          # Generate statistical comparison
          if [ -f "benchmark-results/baseline-${{ matrix.goarch }}.txt" ] && [ -s "benchmark-results/baseline-${{ matrix.goarch }}.txt" ]; then
            echo "📈 Statistical analysis using benchstat..."
            
            # Filter benchmarks for cleaner analysis (remove PASS/FAIL lines)
            echo "🔧 Filtering benchmark data for analysis..."
            benchfilter "*" "benchmark-results/baseline-${{ matrix.goarch }}.txt" > "benchmark-results/baseline-filtered-${{ matrix.goarch }}.txt" 2>/dev/null || cp "benchmark-results/baseline-${{ matrix.goarch }}.txt" "benchmark-results/baseline-filtered-${{ matrix.goarch }}.txt"
            benchfilter "*" "benchmark-results/current-${{ matrix.goarch }}.txt" > "benchmark-results/current-filtered-${{ matrix.goarch }}.txt" 2>/dev/null || cp "benchmark-results/current-${{ matrix.goarch }}.txt" "benchmark-results/current-filtered-${{ matrix.goarch }}.txt"
            
            # Run benchstat on filtered data
            benchstat "benchmark-results/baseline-filtered-${{ matrix.goarch }}.txt" "benchmark-results/current-filtered-${{ matrix.goarch }}.txt" | tee "benchmark-results/comparison-${{ matrix.goarch }}.txt"
            
            # Check for regressions (>10% performance decrease)
            echo "🔍 Checking for performance regressions..."
            
            REGRESSIONS=$(grep -E "~|%" "benchmark-results/comparison-${{ matrix.goarch }}.txt" | grep -E "\+[0-9]+\.[0-9]+%" | wc -l || echo "0")
            IMPROVEMENTS=$(grep -E "~|%" "benchmark-results/comparison-${{ matrix.goarch }}.txt" | grep -E "\-[0-9]+\.[0-9]+%" | wc -l || echo "0")
            
            # Check for significant regressions (>50%)
            CRITICAL_REGRESSIONS=$(grep -E "~|%" "benchmark-results/comparison-${{ matrix.goarch }}.txt" | grep -E "\+[5-9][0-9]\.[0-9]+%|\+[0-9]{3,}\.[0-9]+%" | wc -l || echo "0")
            
            echo "regressions=$REGRESSIONS" >> $GITHUB_OUTPUT
            echo "improvements=$IMPROVEMENTS" >> $GITHUB_OUTPUT
            echo "critical_regressions=$CRITICAL_REGRESSIONS" >> $GITHUB_OUTPUT
            
            # Create enhanced summary
            {
              echo "## 🏁 Benchmark Results Summary - ${{ matrix.label }}"
              echo ""
              echo "**Performance Changes:**"
              echo "- 🔴 Regressions detected: $REGRESSIONS"
              echo "- 🟢 Improvements detected: $IMPROVEMENTS"
              if [ "$CRITICAL_REGRESSIONS" -gt 0 ]; then
                echo "- 🚨 **Critical regressions (>50%)**: $CRITICAL_REGRESSIONS"
              fi
              echo ""
              echo "### 📊 Detailed Statistical Analysis"
              echo "\`\`\`"
              cat "benchmark-results/comparison-${{ matrix.goarch }}.txt"
              echo "\`\`\`"
              echo ""
              echo "### 📈 Current Benchmark Results (Top 20)"
              echo "\`\`\`"
              grep "^Benchmark" "benchmark-results/current-${{ matrix.goarch }}.txt" | head -20
              echo "\`\`\`"
              
              # Add performance insights
              echo ""
              echo "### 🔍 Performance Insights"
              if [ "$CRITICAL_REGRESSIONS" -gt 0 ]; then
                echo "⚠️ **Critical Performance Issues Detected!**"
                echo ""
                echo "The following benchmarks show severe performance regressions (>50%):"
                echo "\`\`\`"
                grep -E "\+[5-9][0-9]\.[0-9]+%|\+[0-9]{3,}\.[0-9]+%" "benchmark-results/comparison-${{ matrix.goarch }}.txt" | head -5
                echo "\`\`\`"
                echo ""
              fi
              
              if [ "$IMPROVEMENTS" -gt 0 ]; then
                echo "✅ **Performance Improvements:**"
                echo "\`\`\`"
                grep -E "\-[0-9]+\.[0-9]+%" "benchmark-results/comparison-${{ matrix.goarch }}.txt" | head -5
                echo "\`\`\`"
                echo ""
              fi
            } > "benchmark-results/summary-${{ matrix.goarch }}.md"
            
            echo "has_comparison=true" >> $GITHUB_OUTPUT
          else
            echo "⚠️ No baseline data available for comparison"
            echo "has_comparison=false" >> $GITHUB_OUTPUT
            echo "regressions=0" >> $GITHUB_OUTPUT
            echo "improvements=0" >> $GITHUB_OUTPUT
            
            # Create summary without comparison
            {
              echo "## 🏁 Benchmark Results - ${{ matrix.label }}"
              echo ""
              echo "**Status:** Baseline benchmarks not available for comparison"
              echo ""
              echo "### 📈 Current Benchmark Results"
              echo "\`\`\`"
              grep "^Benchmark" "benchmark-results/current-${{ matrix.goarch }}.txt" | head -20
              echo "\`\`\`"
            } > "benchmark-results/summary-${{ matrix.goarch }}.md"
          fi

      - name: 📤 Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.goarch }}
          path: |
            benchmark-results/
          retention-days: 30

      - name: ⚠️ Check for performance issues
        if: steps.analysis.outputs.regressions != '0' && steps.analysis.outputs.regressions != ''
        run: |
          REGRESSIONS="${{ steps.analysis.outputs.regressions }}"
          CRITICAL="${{ steps.analysis.outputs.critical_regressions }}"

          if [ "$CRITICAL" != "0" ] && [ "$CRITICAL" != "" ]; then
            echo "🚨 CRITICAL: Severe performance regressions detected!"
            echo "Number of critical regressions (>50%): $CRITICAL"
            echo "Total regressions: $REGRESSIONS"
            echo ""
            echo "⚠️  IMMEDIATE ACTION REQUIRED:"
            echo "- Review the affected benchmark functions"
            echo "- Consider reverting recent changes"
            echo "- Profile the code to identify bottlenecks"
            echo ""
            echo "::error title=Critical Performance Regression::$CRITICAL severe benchmark regressions (>50%) detected for ${{ matrix.label }}"
          else
            echo "⚠️ Performance regressions detected"
            echo "Number of regressions: $REGRESSIONS"
            echo ""
            echo "Please review the benchmark comparison above."
            echo "Consider optimizing the affected code paths."
            echo ""
            echo "::warning title=Performance Regression::$REGRESSIONS benchmark regressions detected for ${{ matrix.label }}"
          fi

  # Combine results and post PR comment
  benchmark-summary:
    name: 📋 Benchmark Summary
    needs: benchmark
    runs-on: ubuntu-latest
    if: always() && github.event_name == 'pull_request'
    steps:
      - name: 📥 Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          path: all-results

      - name: 🔍 Combine results and create PR comment
        run: |
          echo "📊 Creating comprehensive benchmark summary..."

          # Initialize summary
          {
            echo "# 🚀 Benchmark Performance Report"
            echo ""
            echo "**Workflow:** \`${{ github.workflow }}\` | **Run:** [\`${{ github.run_id }}\`](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})"
            echo "**Commit:** \`${{ github.sha }}\` | **Branch:** \`${{ github.head_ref }}\`"
            echo ""
          } > pr-comment.md

          # Add results for each architecture
          total_regressions=0
          total_improvements=0
          total_critical=0

          for arch_dir in all-results/benchmark-results-*; do
            if [ -d "$arch_dir" ]; then
              arch=$(basename "$arch_dir" | sed 's/benchmark-results-//')
              echo "Processing results for $arch..."
              
              if [ -f "$arch_dir/summary-$arch.md" ]; then
                echo "" >> pr-comment.md
                cat "$arch_dir/summary-$arch.md" >> pr-comment.md
                echo "" >> pr-comment.md
                
                # Count regressions and improvements (enhanced parsing)
                if [ -f "$arch_dir/comparison-$arch.txt" ]; then
                  arch_regressions=$(grep -E "\+[0-9]+\.[0-9]+%" "$arch_dir/comparison-$arch.txt" | wc -l || echo "0")
                  arch_improvements=$(grep -E "\-[0-9]+\.[0-9]+%" "$arch_dir/comparison-$arch.txt" | wc -l || echo "0")
                  arch_critical=$(grep -E "\+[5-9][0-9]\.[0-9]+%|\+[0-9]{3,}\.[0-9]+%" "$arch_dir/comparison-$arch.txt" | wc -l || echo "0")
                  
                  total_regressions=$((total_regressions + arch_regressions))
                  total_improvements=$((total_improvements + arch_improvements))
                  total_critical=$((total_critical + arch_critical))
                fi
              fi
            fi
          done

          # Add footer with enhanced summary
          {
            echo "---"
            echo ""
            echo "### 📈 Overall Performance Summary"
            echo "- **Total Regressions:** $total_regressions"
            echo "- **Total Improvements:** $total_improvements"
            if [ $total_critical -gt 0 ]; then
              echo "- **🚨 Critical Regressions (>50%):** $total_critical"
            fi
            echo ""
            if [ $total_critical -gt 0 ]; then
              echo "🚨 **CRITICAL:** Severe performance regressions detected!"
              echo ""
              echo "**Immediate action required:**"
              echo "- Review and optimize critical performance paths"
              echo "- Consider reverting recent changes"
              echo "- Run profiling to identify bottlenecks"
            elif [ $total_regressions -gt 0 ]; then
              echo "⚠️ **Action Required:** Performance regressions detected. Please review and optimize."
            else
              echo "✅ **Excellent!** No performance regressions detected."
              if [ $total_improvements -gt 0 ]; then
                echo "🎉 **Bonus:** Performance improvements detected!"
              fi
            fi
            echo ""
            echo "📝 *This comment will be updated automatically when new commits are pushed.*"
            echo ""
            echo "<!-- benchmark-comment-marker -->"
          } >> pr-comment.md

          echo "Generated PR comment:"
          echo "=========================="
          cat pr-comment.md
          echo "=========================="

      - name: 💬 Post/Update PR comment
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const comment = fs.readFileSync('pr-comment.md', 'utf8');

            // Find existing comment
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const existingComment = comments.data.find(c => 
              c.body.includes('<!-- benchmark-comment-marker -->')
            );

            if (existingComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment
              });
              console.log('Updated existing benchmark comment');
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
              console.log('Created new benchmark comment');
            }

  # Update baseline benchmarks on main branch
  update-baseline:
    name: 📚 Update Baseline
    needs: benchmark
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    steps:
      - name: 📥 Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: 📥 Download benchmark results
        uses: actions/download-artifact@v4
        with:
          path: baseline-update

      - name: 📚 Store baseline benchmarks
        run: |
          echo "📚 Updating baseline benchmarks for future comparisons..."

          # Create baseline directory
          mkdir -p .github/benchmark-baselines

          # Copy current results as new baselines
          for arch_dir in baseline-update/benchmark-results-*; do
            if [ -d "$arch_dir" ]; then
              arch=$(basename "$arch_dir" | sed 's/benchmark-results-//')
              echo "Updating baseline for $arch..."
              
              if [ -f "$arch_dir/current-$arch.txt" ]; then
                cp "$arch_dir/current-$arch.txt" ".github/benchmark-baselines/baseline-$arch.txt"
                echo "✅ Updated baseline for $arch"
              fi
            fi
          done

          # Create metadata
          {
            echo "# Benchmark Baseline Metadata"
            echo "Updated: $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
            echo "Commit: ${{ github.sha }}"
            echo "Workflow: ${{ github.run_id }}"
          } > .github/benchmark-baselines/metadata.txt

          echo "📚 Baseline update completed"

      - name: 💾 Commit baseline updates
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"

          if [ -n "$(git status --porcelain)" ]; then
            git add .github/benchmark-baselines/
            git commit -m "🔄 Update benchmark baselines [skip ci]
            
            Auto-updated from commit ${{ github.sha }}
            Workflow run: ${{ github.run_id }}"
            git push
            echo "✅ Pushed baseline updates"
          else
            echo "📝 No changes to commit"
          fi
